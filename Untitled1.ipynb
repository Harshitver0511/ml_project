{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "849aea84-b91d-4bc0-96b7-75ed20156c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
      "Collecting imbalanced-learn (from imblearn)\n",
      "  Downloading imbalanced_learn-0.14.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.25.2 in c:\\users\\hverm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
      "Requirement already satisfied: scipy<2,>=1.11.4 in c:\\users\\hverm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (1.13.0)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.4.2 in c:\\users\\hverm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: joblib<2,>=1.2.0 in c:\\users\\hverm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\hverm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (3.4.0)\n",
      "Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading imbalanced_learn-0.14.0-py3-none-any.whl (239 kB)\n",
      "Installing collected packages: imbalanced-learn, imblearn\n",
      "\n",
      "   ---------------------------------------- 0/2 [imbalanced-learn]\n",
      "   ---------------------------------------- 0/2 [imbalanced-learn]\n",
      "   ---------------------------------------- 0/2 [imbalanced-learn]\n",
      "   ---------------------------------------- 0/2 [imbalanced-learn]\n",
      "   ---------------------------------------- 0/2 [imbalanced-learn]\n",
      "   ---------------------------------------- 0/2 [imbalanced-learn]\n",
      "   ---------------------------------------- 0/2 [imbalanced-learn]\n",
      "   ---------------------------------------- 0/2 [imbalanced-learn]\n",
      "   ---------------------------------------- 2/2 [imblearn]\n",
      "\n",
      "Successfully installed imbalanced-learn-0.14.0 imblearn-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in c:\\users\\hverm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\hverm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optuna) (1.17.1)\n",
      "Requirement already satisfied: colorlog in c:\\users\\hverm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optuna) (6.10.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\hverm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hverm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optuna) (25.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\hverm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optuna) (2.0.44)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hverm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\hverm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: Mako in c:\\users\\hverm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\hverm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\hverm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\hverm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\hverm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn\n",
    "!pip install optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb9f0ec3-60c5-4d2a-8312-4675361d19c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# --- Base Models ---\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# --- Ensemble & Meta-Model ---\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# --- Pipeline Tools ---\n",
    "# Use the imblearn pipeline to handle SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39042c71-2cae-4360-9a1e-faa33bb45ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-15 19:46:56,145] A new study created in memory with name: no-name-e4fe671d-4934-4d95-8264-7ba0d98f8023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optuna Hyperparameter Search (with parallel fix)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09590bc46984bcea0a749ddbaf0fbd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-15 19:51:36,009] Trial 0 finished with value: 0.9780653230961383 and parameters: {'n_features_to_select': 24, 'knn_n_neighbors': 15, 'rf_n_estimators': 201, 'rf_max_depth': 20, 'meta_C': 3.841218645050506}. Best is trial 0 with value: 0.9780653230961383.\n",
      "[I 2025-11-15 19:55:57,741] Trial 1 finished with value: 0.9746155791524925 and parameters: {'n_features_to_select': 17, 'knn_n_neighbors': 13, 'rf_n_estimators': 188, 'rf_max_depth': 18, 'meta_C': 0.010446709893457029}. Best is trial 0 with value: 0.9780653230961383.\n",
      "[I 2025-11-15 20:00:26,796] Trial 2 finished with value: 0.9786504990011697 and parameters: {'n_features_to_select': 13, 'knn_n_neighbors': 7, 'rf_n_estimators': 134, 'rf_max_depth': 11, 'meta_C': 0.49342831877226245}. Best is trial 2 with value: 0.9786504990011697.\n",
      "[I 2025-11-15 20:05:22,906] Trial 3 finished with value: 0.971380975590462 and parameters: {'n_features_to_select': 14, 'knn_n_neighbors': 7, 'rf_n_estimators': 147, 'rf_max_depth': 26, 'meta_C': 9.659501160403785}. Best is trial 2 with value: 0.9786504990011697.\n",
      "[I 2025-11-15 20:09:04,544] Trial 4 finished with value: 0.9652989929065847 and parameters: {'n_features_to_select': 10, 'knn_n_neighbors': 5, 'rf_n_estimators': 169, 'rf_max_depth': 18, 'meta_C': 3.0906440424278925}. Best is trial 2 with value: 0.9786504990011697.\n",
      "\n",
      "--- Optuna Tuning Complete! ---\n",
      "Best ROC-AUC Score (from CV): 0.9787\n",
      "Best Parameters Found:\n",
      "{'n_features_to_select': 13, 'knn_n_neighbors': 7, 'rf_n_estimators': 134, 'rf_max_depth': 11, 'meta_C': 0.49342831877226245}\n",
      "\n",
      "Training final model with best parameters...\n",
      "\n",
      "--- Final Tuned Model Evaluation on Test Set ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     85295\n",
      "           1       0.11      0.87      0.20       148\n",
      "\n",
      "    accuracy                           0.99     85443\n",
      "   macro avg       0.56      0.93      0.60     85443\n",
      "weighted avg       1.00      0.99      0.99     85443\n",
      "\n",
      "Test Set ROC-AUC Score: 0.9660\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# --- Base Models ---\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# --- Ensemble & Meta-Model ---\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# --- Pipeline Tools ---\n",
    "# SMOTE is removed, so we use the standard sklearn Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from imblearn.over_sampling import SMOTE # <-- SMOTE REMOVED\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "try:\n",
    "    creditcard = pd.read_csv('creditcard.csv', sep=',')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'creditcard.csv' not found.\")\n",
    "    print(\"Please download the dataset from Kaggle and place it in the same directory.\")\n",
    "    # As a fallback, create dummy data to allow the script to run\n",
    "    print(\"Using dummy data to proceed...\")\n",
    "    X_raw = np.random.rand(1000, 30)\n",
    "    y_raw = np.random.randint(0, 2, 1000)\n",
    "    # Ensure at least one of each class for stratify\n",
    "    y_raw[0] = 0\n",
    "    y_raw[1] = 1\n",
    "else:\n",
    "    X_raw = creditcard.drop('Class', axis=1)\n",
    "    y_raw = creditcard['Class']\n",
    "\n",
    "# --- 2. Train/Test Split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, test_size=0.3, stratify=y_raw, random_state=42)\n",
    "\n",
    "\n",
    "# --- 3. Define the Optuna Objective Function ---\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    This function defines the entire ML pipeline and the\n",
    "    hyperparameters that Optuna will tune.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- A. Define the parameter search space ---\n",
    "    n_features = trial.suggest_int('n_features_to_select', 10, 25)\n",
    "    # smote_k = trial.suggest_int('smote_k_neighbors', 3, 7) # <-- SMOTE REMOVED\n",
    "    knn_n = trial.suggest_int('knn_n_neighbors', 3, 15, step=2)\n",
    "    rf_n_est = trial.suggest_int('rf_n_estimators', 100, 300)\n",
    "    rf_max_depth = trial.suggest_int('rf_max_depth', 10, 30)\n",
    "    meta_c = trial.suggest_float('meta_C', 1e-2, 1e1, log=True)\n",
    "\n",
    "    \n",
    "    # --- B. Build the FULL pipeline with these parameters ---\n",
    "    scaler = RobustScaler()\n",
    "    \n",
    "    rfe_estimator = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "    feature_selector = RFE(estimator=rfe_estimator, n_features_to_select=n_features, step=1)\n",
    "\n",
    "    # smote = SMOTE(k_neighbors=smote_k, random_state=42) # <-- SMOTE REMOVED\n",
    "    \n",
    "    # --- FIX 1: Give parallel jobs to the base estimator ---\n",
    "    clf_knn = KNeighborsClassifier(n_neighbors=knn_n)\n",
    "    clf_rf = RandomForestClassifier(\n",
    "        n_estimators=rf_n_est, \n",
    "        max_depth=rf_max_depth, \n",
    "        random_state=42,\n",
    "        class_weight='balanced', # <-- Added to compensate for no SMOTE\n",
    "        n_jobs=-1  # <--- TELL RANDOM FOREST TO USE ALL CORES\n",
    "    )\n",
    "    clf_ada = AdaBoostClassifier(random_state=42)\n",
    "    \n",
    "    meta_model_lr = LogisticRegression(\n",
    "        C=meta_c, \n",
    "        solver='liblinear',\n",
    "        class_weight='balanced' # <-- Added to compensate for no SMOTE\n",
    "    )\n",
    "    \n",
    "    # --- FIX 2: Give parallel jobs to the stacker ---\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=[\n",
    "            ('knn', clf_knn),\n",
    "            ('rf', clf_rf),\n",
    "            ('ada', clf_ada)\n",
    "        ],\n",
    "        final_estimator=meta_model_lr,\n",
    "        cv=3,\n",
    "        n_jobs=-1  # <--- TELL STACKER TO BUILD MODELS IN PARALLEL\n",
    "    )\n",
    "    \n",
    "    # --- C. Create the Final Pipeline ---\n",
    "    pipeline_stack = Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('feature_selector', feature_selector),\n",
    "        # ('smote', smote), # <-- SMOTE REMOVED\n",
    "        ('stacker', stacking_clf)\n",
    "    ])\n",
    "    \n",
    "    # --- D. Evaluate the pipeline ---\n",
    "    # --- FIX 3: Make the outer loop serial ---\n",
    "    score = cross_val_score(\n",
    "        pipeline_stack, \n",
    "        X_train, \n",
    "        y_train, \n",
    "        n_jobs=1,  # <--- RUN 1 CV-FOLD AT A TIME (WAS -1)\n",
    "        cv=3, \n",
    "        scoring='roc_auc'\n",
    "    )\n",
    "    \n",
    "    return np.mean(score)\n",
    "\n",
    "# --- 4. Create and Run the Optuna Study ---\n",
    "print(\"Starting Optuna Hyperparameter Search (with parallel fix)...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=5, show_progress_bar=True) # Using 50 trials as per your code\n",
    "\n",
    "# --- 5. Get the Best Results ---\n",
    "print(\"\\n--- Optuna Tuning Complete! ---\")\n",
    "print(f\"Best ROC-AUC Score (from CV): {study.best_value:.4f}\")\n",
    "print(\"Best Parameters Found:\")\n",
    "print(study.best_params)\n",
    "\n",
    "# --- 6. Train the Final Model with the Best Parameters ---\n",
    "print(\"\\nTraining final model with best parameters...\")\n",
    "best_params = study.best_params\n",
    "\n",
    "# Re-build the *entire* pipeline using the best params\n",
    "final_scaler = RobustScaler()\n",
    "final_rfe_estimator = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "final_feature_selector = RFE(\n",
    "    estimator=final_rfe_estimator,\n",
    "    n_features_to_select=best_params['n_features_to_select'],\n",
    "    step=1\n",
    ")\n",
    "# final_smote = SMOTE(k_neighbors=best_params['smote_k_neighbors'], random_state=42) # <-- SMOTE REMOVED\n",
    "final_knn = KNeighborsClassifier(n_neighbors=best_params['knn_n_neighbors'])\n",
    "final_rf = RandomForestClassifier(\n",
    "    n_estimators=best_params['rf_n_estimators'],\n",
    "    max_depth=best_params['rf_max_depth'],\n",
    "    random_state=42,\n",
    "    class_weight='balanced', # <-- Added to compensate for no SMOTE\n",
    "    n_jobs=-1 # <-- Use all cores in final model\n",
    ")\n",
    "final_ada = AdaBoostClassifier(random_state=42)\n",
    "final_meta_lr = LogisticRegression(\n",
    "    C=best_params['meta_C'], \n",
    "    solver='liblinear',\n",
    "    class_weight='balanced' # <-- Added to compensate for no SMOTE\n",
    ")\n",
    "\n",
    "final_stacker = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('knn', final_knn),\n",
    "        ('rf', final_rf),\n",
    "        ('ada', final_ada)\n",
    "    ],\n",
    "    final_estimator=final_meta_lr,\n",
    "    cv=5, \n",
    "    n_jobs=-1 # <-- Use all cores in final model\n",
    ")\n",
    "\n",
    "final_pipeline = Pipeline([\n",
    "    ('scaler', final_scaler),\n",
    "    ('feature_selector', final_feature_selector),\n",
    "    # ('smote', final_smote), # <-- SMOTE REMOVED\n",
    "    ('stacker', final_stacker)\n",
    "])\n",
    "\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# --- 7. Evaluate the Final Model on Unseen Test Data ---\n",
    "print(\"\\n--- Final Tuned Model Evaluation on Test Set ---\")\n",
    "y_pred_final = final_pipeline.predict(X_test)\n",
    "y_proba_final = final_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(classification_report(y_test, y_pred_final))\n",
    "print(f\"Test Set ROC-AUC Score: {roc_auc_score(y_test, y_proba_final):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927fa20c-98d9-415f-9c0b-a29afbc49fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e1318f1-6874-42e7-b565-77a1bb792374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f4ae52e-fe2c-400a-8922-89be15a228ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FEATURE_NAMES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(final_pipeline, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfraud_detection_pipeline.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Save the list of feature names\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(\u001b[43mFEATURE_NAMES\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_names.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Model and feature names saved successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstreamlit run app.py\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to start the web app.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'FEATURE_NAMES' is not defined"
     ]
    }
   ],
   "source": [
    "joblib.dump(final_pipeline, 'fraud_detection_pipeline.joblib')\n",
    "\n",
    "# Save the list of feature names\n",
    "joblib.dump(FEATURE_NAMES, 'feature_names.joblib')\n",
    "\n",
    "print(\"✅ Model and feature names saved successfully.\")\n",
    "print(\"Run 'streamlit run app.py' to start the web app.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0b6cc10-70c9-4227-93e3-eb824306ce08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-15 20:32:46,634] A new study created in memory with name: no-name-7c8885c8-b8c4-409c-8619-628892757317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optuna Hyperparameter Search (with SMOTE + parallel fix)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a51d4267d8543b8a06eeaaa0f0b277d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-15 20:44:27,599] Trial 0 finished with value: 0.9544549654864901 and parameters: {'n_features_to_select': 11, 'smote_k_neighbors': 5, 'knn_n_neighbors': 15, 'rf_n_estimators': 261, 'rf_max_depth': 29, 'meta_C': 0.032202295300139204}. Best is trial 0 with value: 0.9544549654864901.\n",
      "[I 2025-11-15 20:55:21,814] Trial 1 finished with value: 0.9519918259272111 and parameters: {'n_features_to_select': 20, 'smote_k_neighbors': 4, 'knn_n_neighbors': 9, 'rf_n_estimators': 175, 'rf_max_depth': 20, 'meta_C': 3.5931387691795122}. Best is trial 0 with value: 0.9544549654864901.\n",
      "[I 2025-11-15 21:30:24,916] Trial 2 finished with value: 0.9661210371322914 and parameters: {'n_features_to_select': 21, 'smote_k_neighbors': 7, 'knn_n_neighbors': 13, 'rf_n_estimators': 256, 'rf_max_depth': 20, 'meta_C': 0.03409615297401364}. Best is trial 2 with value: 0.9661210371322914.\n",
      "[I 2025-11-15 21:40:59,972] Trial 3 finished with value: 0.9732313316179869 and parameters: {'n_features_to_select': 22, 'smote_k_neighbors': 4, 'knn_n_neighbors': 7, 'rf_n_estimators': 197, 'rf_max_depth': 10, 'meta_C': 0.034894222014854905}. Best is trial 3 with value: 0.9732313316179869.\n",
      "[I 2025-11-15 21:53:13,484] Trial 4 finished with value: 0.9544645515504998 and parameters: {'n_features_to_select': 10, 'smote_k_neighbors': 7, 'knn_n_neighbors': 15, 'rf_n_estimators': 291, 'rf_max_depth': 22, 'meta_C': 0.017255648321189494}. Best is trial 3 with value: 0.9732313316179869.\n",
      "[I 2025-11-15 22:02:48,945] Trial 5 finished with value: 0.9653752977691376 and parameters: {'n_features_to_select': 14, 'smote_k_neighbors': 7, 'knn_n_neighbors': 5, 'rf_n_estimators': 175, 'rf_max_depth': 28, 'meta_C': 1.4722548882600426}. Best is trial 3 with value: 0.9732313316179869.\n",
      "[I 2025-11-15 22:11:04,364] Trial 6 finished with value: 0.961071103096247 and parameters: {'n_features_to_select': 11, 'smote_k_neighbors': 6, 'knn_n_neighbors': 7, 'rf_n_estimators': 172, 'rf_max_depth': 17, 'meta_C': 0.07019898331280404}. Best is trial 3 with value: 0.9732313316179869.\n",
      "[I 2025-11-15 22:30:01,379] Trial 7 finished with value: 0.961262390135769 and parameters: {'n_features_to_select': 20, 'smote_k_neighbors': 3, 'knn_n_neighbors': 13, 'rf_n_estimators': 247, 'rf_max_depth': 20, 'meta_C': 0.06407908384605746}. Best is trial 3 with value: 0.9732313316179869.\n",
      "[I 2025-11-15 22:43:14,066] Trial 8 finished with value: 0.9622530956607284 and parameters: {'n_features_to_select': 16, 'smote_k_neighbors': 5, 'knn_n_neighbors': 13, 'rf_n_estimators': 228, 'rf_max_depth': 21, 'meta_C': 0.5716334370507861}. Best is trial 3 with value: 0.9732313316179869.\n",
      "[I 2025-11-15 22:59:54,007] Trial 9 finished with value: 0.9641276338727959 and parameters: {'n_features_to_select': 22, 'smote_k_neighbors': 4, 'knn_n_neighbors': 15, 'rf_n_estimators': 294, 'rf_max_depth': 21, 'meta_C': 3.041338226440248}. Best is trial 3 with value: 0.9732313316179869.\n",
      "\n",
      "--- Optuna Tuning Complete! ---\n",
      "Best ROC-AUC Score (from CV): 0.9732\n",
      "Best Parameters Found:\n",
      "{'n_features_to_select': 22, 'smote_k_neighbors': 4, 'knn_n_neighbors': 7, 'rf_n_estimators': 197, 'rf_max_depth': 10, 'meta_C': 0.034894222014854905}\n",
      "\n",
      "Training final model with best parameters...\n",
      "\n",
      "--- Final Tuned Model Evaluation on Test Set ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85295\n",
      "           1       0.44      0.84      0.57       148\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.72      0.92      0.79     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n",
      "Test Set ROC-AUC Score: 0.9651\n",
      "\n",
      "--- Saving final model and feature names... ---\n",
      "✅ Model and feature names saved successfully.\n",
      "Run 'streamlit run app.py' to start the web app.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import joblib  # For saving the model\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# --- Base Models ---\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# --- Ensemble & Meta-Model ---\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# --- Pipeline Tools ---\n",
    "# We MUST use the imblearn Pipeline to include SMOTE\n",
    "from imblearn.pipeline import Pipeline \n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "try:\n",
    "    creditcard = pd.read_csv('creditcard.csv', sep=',')\n",
    "    X_raw = creditcard.drop('Class', axis=1)\n",
    "    y_raw = creditcard['Class']\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'creditcard.csv' not found.\")\n",
    "    print(\"Please download the dataset from Kaggle and place it in the same directory.\")\n",
    "    # As a fallback, create dummy data to allow the script to run\n",
    "    print(\"Using dummy data to proceed...\")\n",
    "    X_raw = pd.DataFrame(np.random.rand(1000, 30), columns=[f'V{i}' for i in range(29)] + ['Amount'])\n",
    "    X_raw.rename(columns={'V0':'Time'}, inplace=True)\n",
    "    y_raw = pd.Series(np.random.randint(0, 2, 1000))\n",
    "    y_raw.iloc[0] = 0\n",
    "    y_raw.iloc[1] = 1\n",
    "\n",
    "\n",
    "# --- 2. Train/Test Split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, test_size=0.3, stratify=y_raw, random_state=42)\n",
    "\n",
    "# Save feature names - this is CRITICAL for deployment\n",
    "FEATURE_NAMES = X_train.columns.to_list()\n",
    "\n",
    "\n",
    "# --- 3. Define the Optuna Objective Function ---\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    This function defines the entire ML pipeline and the\n",
    "    hyperparameters that Optuna will tune.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- A. Define the parameter search space ---\n",
    "    n_features = trial.suggest_int('n_features_to_select', 10, 25)\n",
    "    smote_k = trial.suggest_int('smote_k_neighbors', 3, 7) # <-- SMOTE parameter\n",
    "    knn_n = trial.suggest_int('knn_n_neighbors', 3, 15, step=2)\n",
    "    rf_n_est = trial.suggest_int('rf_n_estimators', 100, 300)\n",
    "    rf_max_depth = trial.suggest_int('rf_max_depth', 10, 30)\n",
    "    meta_c = trial.suggest_float('meta_C', 1e-2, 1e1, log=True)\n",
    "\n",
    "    \n",
    "    # --- B. Build the FULL pipeline with these parameters ---\n",
    "    scaler = RobustScaler()\n",
    "    \n",
    "    # RFE estimator (class_weight='balanced' helps RFE itself)\n",
    "    rfe_estimator = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "    feature_selector = RFE(estimator=rfe_estimator, n_features_to_select=n_features, step=1)\n",
    "\n",
    "    # SMOTE object\n",
    "    smote = SMOTE(k_neighbors=smote_k, random_state=42)\n",
    "\n",
    "    # Base Models (n_jobs=-1 for parallelism)\n",
    "    clf_knn = KNeighborsClassifier(n_neighbors=knn_n)\n",
    "    clf_rf = RandomForestClassifier(\n",
    "        n_estimators=rf_n_est, \n",
    "        max_depth=rf_max_depth, \n",
    "        random_state=42,\n",
    "        n_jobs=-1  # <-- Use all cores\n",
    "    )\n",
    "    clf_ada = AdaBoostClassifier(random_state=42)\n",
    "    \n",
    "    # Meta Model (no class_weight needed, SMOTE handles it)\n",
    "    meta_model_lr = LogisticRegression(\n",
    "        C=meta_c, \n",
    "        solver='liblinear'\n",
    "    )\n",
    "    \n",
    "    # Stacking Classifier (n_jobs=-1 for parallelism)\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=[\n",
    "            ('knn', clf_knn),\n",
    "            ('rf', clf_rf),\n",
    "            ('ada', clf_ada)\n",
    "        ],\n",
    "        final_estimator=meta_model_lr,\n",
    "        cv=3,\n",
    "        n_jobs=-1 # <-- Use all cores\n",
    "    )\n",
    "    \n",
    "    # --- C. Create the Final Pipeline ---\n",
    "    # The order is critical: Scale -> Select Features -> Oversample -> Stack\n",
    "    pipeline_stack = Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('feature_selector', feature_selector),\n",
    "        ('smote', smote),\n",
    "        ('stacker', stacking_clf)\n",
    "    ])\n",
    "    \n",
    "    # --- D. Evaluate the pipeline ---\n",
    "    # n_jobs=1 forces the outer CV to be serial\n",
    "    score = cross_val_score(\n",
    "        pipeline_stack, \n",
    "        X_train, \n",
    "        y_train, \n",
    "        n_jobs=1,  # <-- Run 1 CV-fold at a time (THE FIX)\n",
    "        cv=3, \n",
    "        scoring='roc_auc'\n",
    "    )\n",
    "    \n",
    "    return np.mean(score)\n",
    "\n",
    "# --- 4. Create and Run the Optuna Study ---\n",
    "print(\"Starting Optuna Hyperparameter Search (with SMOTE + parallel fix)...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "# Using n_trials=50. Increase if you have time.\n",
    "study.optimize(objective, n_trials=10, show_progress_bar=True) \n",
    "\n",
    "# --- 5. Get the Best Results ---\n",
    "print(\"\\n--- Optuna Tuning Complete! ---\")\n",
    "print(f\"Best ROC-AUC Score (from CV): {study.best_value:.4f}\")\n",
    "print(\"Best Parameters Found:\")\n",
    "print(study.best_params)\n",
    "\n",
    "# --- 6. Train the Final Model with the Best Parameters ---\n",
    "print(\"\\nTraining final model with best parameters...\")\n",
    "best_params = study.best_params\n",
    "\n",
    "# Re-build the *entire* pipeline using the best params\n",
    "final_scaler = RobustScaler()\n",
    "final_rfe_estimator = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "final_feature_selector = RFE(\n",
    "    estimator=final_rfe_estimator,\n",
    "    n_features_to_select=best_params['n_features_to_select'],\n",
    "    step=1\n",
    ")\n",
    "final_smote = SMOTE(k_neighbors=best_params['smote_k_neighbors'], random_state=42)\n",
    "final_knn = KNeighborsClassifier(n_neighbors=best_params['knn_n_neighbors'])\n",
    "final_rf = RandomForestClassifier(\n",
    "    n_estimators=best_params['rf_n_estimators'],\n",
    "    max_depth=best_params['rf_max_depth'],\n",
    "    random_state=42,\n",
    "    n_jobs=-1 # <-- Use all cores\n",
    ")\n",
    "final_ada = AdaBoostClassifier(random_state=42)\n",
    "final_meta_lr = LogisticRegression(\n",
    "    C=best_params['meta_C'], \n",
    "    solver='liblinear'\n",
    ")\n",
    "\n",
    "final_stacker = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('knn', final_knn),\n",
    "        ('rf', final_rf),\n",
    "        ('ada', final_ada)\n",
    "    ],\n",
    "    final_estimator=final_meta_lr,\n",
    "    cv=5, \n",
    "    n_jobs=-1 # <-- Use all cores\n",
    ")\n",
    "\n",
    "final_pipeline = Pipeline([\n",
    "    ('scaler', final_scaler),\n",
    "    ('feature_selector', final_feature_selector),\n",
    "    ('smote', final_smote),\n",
    "    ('stacker', final_stacker)\n",
    "])\n",
    "\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# --- 7. Evaluate the Final Model on Unseen Test Data ---\n",
    "print(\"\\n--- Final Tuned Model Evaluation on Test Set ---\")\n",
    "y_pred_final = final_pipeline.predict(X_test)\n",
    "y_proba_final = final_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(classification_report(y_test, y_pred_final))\n",
    "print(f\"Test Set ROC-AUC Score: {roc_auc_score(y_test, y_proba_final):.4f}\")\n",
    "\n",
    "\n",
    "# --- 8. SAVE THE FINAL MODEL AND FEATURE NAMES ---\n",
    "print(\"\\n--- Saving final model and feature names... ---\")\n",
    "\n",
    "# Save the trained pipeline\n",
    "joblib.dump(final_pipeline, 'fraud_detection_pipeline.joblib')\n",
    "\n",
    "# Save the list of feature names\n",
    "joblib.dump(FEATURE_NAMES, 'feature_names.joblib')\n",
    "\n",
    "print(\"✅ Model and feature names saved successfully.\")\n",
    "print(\"Run 'streamlit run app.py' to start the web app.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fee45cd-83b9-4548-bb0d-0ae393e8926e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
